---
title: "Research_01"
layout: single-portfolio
excerpt: "<img src='/images/research/ecdf.png' alt=''>"
collection: research
order_number: 10
header: 
  og_image: "research/ecdf.png"
---


### Title: Depp Learning Based Rain Attenuation Prediction Modeling for Microwave and mmWave Band "
## Type: "Master Degree Thesis"
## Research Lab: "Beijing Jiaotong University, School of Electronics and Information Engineering, Modern Communication Research Institute"
## General research information 
Rain attenuation is one of the major challenges for wireless communication systems operating in microwave and millimeter wave bands. Accurate prediction of rain attenuation can help improve the performance and reliability of these systems. However, existing methods based on empirical and stochastic models have limitations in terms of accuracy, applicability, and complexity. Therefore, there is a need for novel methods that can leverage the power of data and artificial intelligence to model and predict rain attenuation.
In recent years, deep learning has emerged as a promising technique for various applications in wireless communications, such as channel estimation, signal detection, and beamforming. Deep learning can learn complex and nonlinear patterns from large-scale data, and provide high-performance and robust solutions. In particular, recurrent neural networks (RNNs) and their variants, such as long short-term memory (LSTM) and gated recurrent unit (GRU), are suitable for modeling sequential and temporal data, such as rain attenuation time series.
Several studies have proposed deep learning-based methods for rain attenuation prediction using different types of data and architectures. For example, [This](https://www.ejece.org/index.php/ejece/article/view/498) research has proposed a method based on LSTM network that uses synthetic rain attenuation data generated by the ITU-R P.1853 recommendation. The result showed that the method outperforms existing empirical and stochastic models in terms of accuracy and root mean square error.
Another type of data that can be used for rain attenuation prediction is satellite and radar imagery data, which can provide spatial and temporal information about the rainfall distribution and intensity. For example, [This](https://arxiv.org/abs/2110.00695) has proposed a deep learning-based architecture that forecasts future rain fade using satellite and radar imagery data as well as link power measurements. They explained the data preprocessing and architectural design in detail, and conducted multiple experiments to evaluate their method. They claimed that their method can provide reliable and accurate rain fade prediction for satellite communications.
In this research, we propose a deep learning-based method for rain attenuation prediction modeling using historical data of rainfall rate, path distance, and frequency. We use a long short-term memory (LSTM) network, which is a type of recurrent neural network, to capture the temporal dynamics of rain attenuation.  we put our research base on the ITU-R P.1853, ITU-R P.530, ITU-R P.618 and ITU-R P.838 to study the existing models and generate synthetic data using those models and local weather data such as Rain, Temperature and Relative Humidity, and radio wave propagation data from Ethiopia as our research data. 
We plan to do design a model that can fit with the existing local data and models so as to predict the rain attenuation more accurately and precisely so that it can contribute a lot to the researchers, microwave and mmWave link designers in the region. 
